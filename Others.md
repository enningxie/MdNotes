# FromWechat
## 1. 神经网络浅讲：从神经元到深度学习

神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。

设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定。

事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。

与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。

我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。

两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。

在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。

机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。

下面的问题就是求：如何优化参数，能够让损失函数的值最小。

一般来说解决这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用反向传播算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

反向传播算法的启示是数学中的链式法则。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。

神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。

90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。

2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。

与两层层神经网络不同。多层神经网络中的层数增加了很多。增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。

通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。

在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。

在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。

在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现过拟合现象。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。

多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。

冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。

下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。

从单层神经网络，到两层神经网络，再到多层神经网络，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。可以看出，随着层数增加，其非线性分界拟合能力不断增强。神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。

当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。

但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。

互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。

除此以外，一门技术的发扬没有“伯乐”也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的Rosenblat，Rumelhart没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。

目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。

神经网络强大预测能力的根本，就是多层的神经网络可以无限逼近真实的对应函数，从而模拟数据之间的真实关系。

---

## 2. 如何长时间高效学习

你只需要20小时就能不错的掌握一个全新的知识和技能。

如果我们能在最初的20小时内熬过初学阶段的不适感和挫败感，我们其实能很轻松地掌握一个全新的领域。

一天高效学习时间安排在八个小时左右，是可以执行的。

学习仪式感：人，藉由这种仪式带来的仪式感，来给自己一种强烈的自我暗示---------这种自我暗示能够使自我变革，把自己的专注力、反应能力、运动能力迅速提升。翻看即将在要来到的两个小时之内需要学习的内容，心里有个大概。但是我不会把这个活动当做是我的正式学习的过程。我现在做的活动，只是为了等一下正式学习更好的强化效果。

第一遍阅读：
1. 看目录：知道这一章重点在哪一节，这一节大概用来解决什么问题
2. 看章后习题，圈出术语：这个术语基本上就是本章的知识点了
3. 根据术语去书中划概念和术语解释：如果有些术语不能理解，请用网络搜索术语名词解释
4. 术语理解后带着术语去理解书中的图表和例题以及案例

工科的书，我认为就是一份份的说明书。

看书看不进，就牢牢抓住书本的例题、案例、图表。因为例题讲具体情境、图表具有可视化、案例就是讲具体的运用——这些都比理解文字描述容易的多。而且，例题里面包含了对关键知识点的运用，案例和图表其实都是为了辅助你理解正文文字内容的。

所以，只要我们配合最少量的文字看懂了案例、图表，就达到了对知识的了解，接着我们再去看例题就知道了知识运用场景，之后，我们再反复地做题目，从而达到了对知识点的掌握。

主动掌握的东西不由自主地就比“课标要求”的多得多。

身体的直接能量是葡萄糖，它把葡萄糖当资金一样运用。

完成规定任务量之后，在某一个热情高涨的时候索性中断学习。

在跑步感觉良好的时候主动结束，这样他就会对第二天的跑步充满期待。

当时间一长，我们能记住关于一件事物的主要部分其实是事物留给我们的感觉而不是事件本身。

学习它是一个长期的过程。

无论是什么学习，如果有这件事情来说对你重要，也有必要，但是你却讨厌它，那么请你在讨厌的时候立马去学习它，学到你喜欢它的时候立刻中断它。

因为人都是追求快乐的，你学到情绪快乐的时候就中断，你的情绪就会带领你再来领略一次那种似曾相识的快乐，即便你的情绪不会主动带你来，也不要紧，因为你至少不会在心理上抗拒了。

朋友，记住一条简单的惯性定律:静止的倾向于静止，运动的倾向于运动。不知道一件事情怎么开始才合适？先去做10分钟。不知道怎么和女孩搭讪？立马过去说一声:“你好！……”后面的你自然会接着说。动起来，运动的倾向于运动。我真喜欢这句话。

每个人的高能学习时间段可能不一样。但是我要强调一个词「预计」。观察自己长期的活动状态，我认为，预计自己能量爆发态出现在上午8~10，下午2~5点，晚上6：30~10点，一共八个半小时。

所以，我会为了这八个小时做了很多额外准备，比如，中午午睡就是必要工作；下午5点40左右，我可能会眯一会，这也是必要的工作；休息间隙用牛逼的耳机听喜欢的音乐，是必要的工作；个人爱好，俯卧撑也是必要工作；巧克力也是……

总之，都是为了保证这八个半小时的高效利用而准备（实际上达不到8个半，8~10点休息10分钟，下午2~5点休息10*2，晚上6：30到10点，10*2，因此学习纯时间是7小时40分钟）。

因为里面提到下午5点40左右的小睡，那个时候并不困，是为了防止晚上三个半小时产生疲劳而提前进行的小睡眠的。在疲劳来临之前休息放松是最好的缓解疲劳的方法，你没有听说过吗？你听说过，所以，我再一次提醒你而已。

ps.别以为这样学习苦逼，其实我是在极大愉悦度中进行的，伴随着强烈的仪式感，每一天都像朝圣之行。

---

## 3. 如何退出 Vim？答案来了

点击 Esc 键，Vim 进入命令模式。然后输入：

:q  ——退出（这是 :quit 的缩写）  
:q! ——不保存退出（这是  :quit! 的缩写）  
:wq ——写入文件并退出；（这是 :writequit 的缩写）  
:wq! ——（如果文件只有读权限）写入并退出（如果文件没有写权限，强制写）  
:x ——类似于 :wq，如果文件无变动，那就不写入  
:qa ——退出全部（这是 :quitall 的缩写）

其实 Vim 有很详细的帮助，进入命令模式后，输入 help 然后回车。

---
