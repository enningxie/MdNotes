# FromWechat
## 1. 神经网络浅讲：从神经元到深度学习

神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。

设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定。

事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。

与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。

我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。

两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。

在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。

机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。

下面的问题就是求：如何优化参数，能够让损失函数的值最小。

一般来说解决这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用反向传播算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

反向传播算法的启示是数学中的链式法则。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。

神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。

90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。

2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。

与两层层神经网络不同。多层神经网络中的层数增加了很多。增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。

通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。

在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。

在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。

在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现过拟合现象。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。

多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。

冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。

下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。

从单层神经网络，到两层神经网络，再到多层神经网络，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。可以看出，随着层数增加，其非线性分界拟合能力不断增强。神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。

当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。

但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。

互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。

除此以外，一门技术的发扬没有“伯乐”也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的Rosenblat，Rumelhart没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。

目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。

神经网络强大预测能力的根本，就是多层的神经网络可以无限逼近真实的对应函数，从而模拟数据之间的真实关系。

---

## 2. 如何长时间高效学习

你只需要20小时就能不错的掌握一个全新的知识和技能。

如果我们能在最初的20小时内熬过初学阶段的不适感和挫败感，我们其实能很轻松地掌握一个全新的领域。

一天高效学习时间安排在八个小时左右，是可以执行的。

学习仪式感：人，藉由这种仪式带来的仪式感，来给自己一种强烈的自我暗示---------这种自我暗示能够使自我变革，把自己的专注力、反应能力、运动能力迅速提升。翻看即将在要来到的两个小时之内需要学习的内容，心里有个大概。但是我不会把这个活动当做是我的正式学习的过程。我现在做的活动，只是为了等一下正式学习更好的强化效果。

第一遍阅读：
1. 看目录：知道这一章重点在哪一节，这一节大概用来解决什么问题
2. 看章后习题，圈出术语：这个术语基本上就是本章的知识点了
3. 根据术语去书中划概念和术语解释：如果有些术语不能理解，请用网络搜索术语名词解释
4. 术语理解后带着术语去理解书中的图表和例题以及案例

工科的书，我认为就是一份份的说明书。

看书看不进，就牢牢抓住书本的例题、案例、图表。因为例题讲具体情境、图表具有可视化、案例就是讲具体的运用——这些都比理解文字描述容易的多。而且，例题里面包含了对关键知识点的运用，案例和图表其实都是为了辅助你理解正文文字内容的。

所以，只要我们配合最少量的文字看懂了案例、图表，就达到了对知识的了解，接着我们再去看例题就知道了知识运用场景，之后，我们再反复地做题目，从而达到了对知识点的掌握。

主动掌握的东西不由自主地就比“课标要求”的多得多。

身体的直接能量是葡萄糖，它把葡萄糖当资金一样运用。

完成规定任务量之后，在某一个热情高涨的时候索性中断学习。

在跑步感觉良好的时候主动结束，这样他就会对第二天的跑步充满期待。

当时间一长，我们能记住关于一件事物的主要部分其实是事物留给我们的感觉而不是事件本身。

学习它是一个长期的过程。

无论是什么学习，如果有这件事情来说对你重要，也有必要，但是你却讨厌它，那么请你在讨厌的时候立马去学习它，学到你喜欢它的时候立刻中断它。

因为人都是追求快乐的，你学到情绪快乐的时候就中断，你的情绪就会带领你再来领略一次那种似曾相识的快乐，即便你的情绪不会主动带你来，也不要紧，因为你至少不会在心理上抗拒了。

朋友，记住一条简单的惯性定律:静止的倾向于静止，运动的倾向于运动。不知道一件事情怎么开始才合适？先去做10分钟。不知道怎么和女孩搭讪？立马过去说一声:“你好！……”后面的你自然会接着说。动起来，运动的倾向于运动。我真喜欢这句话。

每个人的高能学习时间段可能不一样。但是我要强调一个词「预计」。观察自己长期的活动状态，我认为，预计自己能量爆发态出现在上午8~10，下午2~5点，晚上6：30~10点，一共八个半小时。

所以，我会为了这八个小时做了很多额外准备，比如，中午午睡就是必要工作；下午5点40左右，我可能会眯一会，这也是必要的工作；休息间隙用牛逼的耳机听喜欢的音乐，是必要的工作；个人爱好，俯卧撑也是必要工作；巧克力也是……

总之，都是为了保证这八个半小时的高效利用而准备（实际上达不到8个半，8~10点休息10分钟，下午2~5点休息10*2，晚上6：30到10点，10*2，因此学习纯时间是7小时40分钟）。

因为里面提到下午5点40左右的小睡，那个时候并不困，是为了防止晚上三个半小时产生疲劳而提前进行的小睡眠的。在疲劳来临之前休息放松是最好的缓解疲劳的方法，你没有听说过吗？你听说过，所以，我再一次提醒你而已。

ps.别以为这样学习苦逼，其实我是在极大愉悦度中进行的，伴随着强烈的仪式感，每一天都像朝圣之行。

---

## 3. 如何退出 Vim？答案来了

点击 Esc 键，Vim 进入命令模式。然后输入：

:q  ——退出（这是 :quit 的缩写）  
:q! ——不保存退出（这是  :quit! 的缩写）  
:wq ——写入文件并退出；（这是 :writequit 的缩写）  
:wq! ——（如果文件只有读权限）写入并退出（如果文件没有写权限，强制写）  
:x ——类似于 :wq，如果文件无变动，那就不写入  
:qa ——退出全部（这是 :quitall 的缩写）

其实 Vim 有很详细的帮助，进入命令模式后，输入 help 然后回车。

---

## 4. AlphaGo之父的演讲

DeepMind现在在努力制造世界上第一台通用学习机，大体上学习可以分为两类：一种就是直接从输入和经验中学习，没有既定的程序或者规则可循，系统需要从原始数据自己进行学习；第二种学习系统就是通用学习系统，指的是一种算法可以用于不同的任务和领域，甚至是一些从未见过的全新领域。

在这里我解释一下增强学习，首先，想像一下有一个主体，在AI领域我们称我们的人工智能系统为主体，它需要了解自己所处的环境，并尽力找出自己要达到的目的。这里的环境可以指真实事件，可以是机器人，也可以是虚拟世界，比如游戏环境；主体通过两种方式与周围环境接触；它先通过观察熟悉环境，我们起初通过视觉，也可以通过听觉、触觉等，我们也在发展多感觉的系统；

第二个任务，就是在此基础上，建模并找出最佳选择。这可能涉及到对未来的预期，想像，以及假设检验。这个主体经常处在真实环境中，当时间节点到了的时候，系统需要输出当前找到的最佳方案。这个方案可能或多或少会改变所处环境，从而进一步驱动观察的结果，并反馈给主体。

另外，从生物角度来讲，动物和人类等，人类的大脑是多巴胺控制的，它在执行增强学习的行为。因此，不论是从数学的角度，还是生物的角度，增强学习是一个有效的解决人工智能问题的工具。

围棋就不同了，如果你去问世界级的大师，为什么走这一步，他们经常回答你直觉告诉他这么走，这是真的，他们是没法描述其中的原因的。我们通过用加强学习的方式来提高人工神经网络算法，希望能够解决这一问题。我们试图通过深度神经网络模仿人类的这种直觉行为，在这里，需要训练两个神经网络，一种是决策网络，我们从网上下载了成百万的业余围棋游戏，通过监督学习，我们让阿尔法狗模拟人类下围棋的行为；我们从棋盘上任意选择一个落子点，训练系统去预测下一步人类将作出的决定；系统的输入是在那个特殊位置最有可能发生的前五或者前十的位置移动；这样，你只需看那5-10种可能性，而不用分析所有的200种可能性了。

一旦我们有了这个，我们对系统进行几百万次的训练，通过误差加强学习，对于赢了的情况，让系统意识到，下次出现类似的情形时，更有可能做相似的决定。相反，如果系统输了，那么下次再出现类似的情况，就不会选择这种走法。我们建立了自己的游戏数据库，通过百万次的游戏，对系统进行训练，得到第二种神经网络。选择不同的落子点，经过置信区间进行学习，选出能够赢的情况，这个几率介于0-1之间，0是根本不可能赢，1是百分之百赢。

通过把这两个神经网络结合起来（决策网络和数值网络），我们可以大致预估出当前的情况。这两个神经网络树，通过蒙特卡洛算法，把这种本来不能解决的问题，变得可以解决。

直觉是一种含蓄的表达，它是基于人类的经历和本能的一种思维形式，不需要精确计算。这一决策的准确性可以通过行为进行评判。在围棋里很简单，我们给系统输入棋子的位置，来评估其重要性。阿尔法狗就是在模拟人类这种直觉行为。创新，我认为就是在已有知识和经验的基础上，产生一种原始的，创新的观点。阿尔法狗很明显的示范了这两种能力。

国际象棋更注重战术，而阿尔法狗更注重战略。

在如今这个充斥着各种新技术的时代，人工智能必须在人类道德基准范围内被开发和利用。

---

## 5. Linux目录结构详细介绍

|目录|描述|    
|---|---|
|/|第一层结构的根、整个文件系统层次结构的根目录。|
|/bin/|需要在单用户模式可用的必要命令（可执行文件）；面向所有用户，例如：cat\ls\cp，和/usr/bin类似|
|/boot/|引导程序文件，例如：kernel\initrd；时常是一个单独的分区|
|/dev/|必要设备，例如：/dev/null|
|/etc/|特定主机，系统范围内的配置文件。可编辑的文本配置、扩展工具箱|
|/etc/opt/|/opt/的配置文件|
|/etc/X11/|X_Window系统（版本11）的配置文件|
|/etc/sgml/|SGML的配置文件|
|/etc/xml|XML的配置文件|
|/home/|用户的home目录，包含保存的文件、个人设置等，一般为单独的分区。|
|/lib/|/bin/ and /sbin/ 中二进制文件必要的库文件。|
|/media/|可移除媒体（如CD-ROM）的挂载点。|
|/lost+found|在ext3文件系统中，当系统意外崩溃或机器意外关机，会产生一些文件碎片在这里。当系统在开机启动的过程中fsck工具会检查这里，并修复已经损坏的文件系统。当系统发生问题。可能会有文件被移动到这个目录中，可能需要用手工的方式修复，或移动文件到原来的位置上。|
|/mnt/|临时挂载的文件系统。比如cdrom\u盘等，直接插入光驱无法使用，要先挂载后使用。|
|/opt/|可选应用软件包。|
|/proc/|虚拟文件系统，将内核与进程状态归档为文本文件（系统信息都存放在该目录下）。例如：uptime、 network。在Linux中，对应Procfs格式挂载。该目录下文件只能看不能改（包括root）|
|/root/|超级用户的home目录|
|/sbin/|必要的系统二进制文件，例如：init\ip\mount。sbin目录下的命令，普通用户都执行不了。|
|/srv/|站点的具体数据，由系统提供。|
|/tmp/|临时文件（参见/var/tmp），在系统重启时目录中文件不会被保留。|
|/usr/|默认软件都会存于该目录下。用于存储只读用户数据的第二层次；包含绝大多数的用户工具和应用程序。|
|/var/|变量文件--在正常运行的系统中其内容不断变化的文件，如日志，脱机文件和临时电子邮件文件。有时是一个单独的分区。如果不单独分区，有可能会把整个分区充满。|

---

## 6. 写给 Git 初学者的7个建议

###### 第一条：花时间去学习Git的基本操作

###### 第二条：从简单的Git工作流开始

少即是多。

总的来说：不要因为觉得Git什么都要学就压力很大，你完全可以从今天开始使用Git。

###### 第三条：不要再害怕犯错误

Git最出色的一点是：它几乎是100%易上手误操作的。

Git基本上不删除数据。即使是那些看起来是删除数据的操作，实际上是为了让你更快的撤销删除，而在向系统添加数据。

Git基本可以撤销所有操作。我鼓励你更多的实验和探索你的想法，因为这就是使用版本控制系统系统的最主要的好处之一。

你团队的每一个成员都在他／她的计算机中有各自的副本。本质上这更像是整个版本控制项目中的冗余备份（包括包括整个历史记录），你捅了大娄子而且还没办法还原这种情况是极其少见的。

###### 第四条：理解分支概念

在Git里面，分支这个概念是你一开始能学到的最有用的东西了。分支允许你隔离开发你的项目，而要想成为一个高效的Git用户，这是非常关键的一点。

一开始这听起来好像不是什么大事，但一旦你完全的理解了分支概念，你会开始想没有这个你怎么活下去。

尽管其他的版本控制系统也会使用分支概念，Git是第一个实现它，并让它变的好用的系统。

###### 第五条：学习暂存区

当你的提交里面只包含一些相关的变化时，版本控制会变的非常有用，它保证了你的提交可以被没有任何副作用的回滚，经常提交的习惯也可以让你的同事更好的了解你的进度。

Git有个功能叫暂存区让这一切都变为可能。

学习使用暂存区，并爱上它，因为这是Git里面最重要最独立的一个模块。

###### 第六条：用Git图形界面

尽管使用图形界面绝对不会是一个要求，但我还是高度推荐使用。

使用图形界面让大多数操作都变得简单，让你在项目开始时便占尽优势。

不管怎么说，使用Git不应该只是记住各种命令和参数，而是改进你的编程工作流。如果图形界面可以做到这一点的话，没有理由让简单的事变的困难嘛。

使用图形界面并不能减轻你学习Git基础的负担，不过一旦你快乐的征服了Git，使用这些工具会让你的生活变得更轻松。

###### 第七条：对自己承诺你会用Git

使用一个新工具一开始会让人非常头疼，走过这条学习曲线的方法只有一个：继续走下去。

做一个充分的承诺，不要回头。在你平常的工作流里引入Git很快就会被证明这是你近期做的最大的，最有意义的决定。

充分承诺的这种心态会让你有更多的机会去练习，让事情变得更加简单，因为你知道你现在这个项目用了版本控制系统。而更重要的是，让Git成为你的编程习惯。

对自己做一个100%的承诺，作为Git征服之路的开始。

[原文链接](http://blog.jobbole.com/50603/)

---

## 7.
